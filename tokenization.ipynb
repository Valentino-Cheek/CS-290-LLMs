{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f592fbb0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "_Tokenization_ is the process of converting a body of text into individual _tokens_, e.g., words and punctuation characters. This is the first step for most Natural Language Processing (NLP) tasks, including preparing data for training an LLM. Let's see how it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f17e49",
   "metadata": {},
   "source": [
    "## Some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc42f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test! Or is this not a test? Test it to be sure. :)\n",
      "This sample text has 61 characters.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test! Or is this not a test? Test it to be sure. :)\"\n",
    "print(text)\n",
    "print(f\"This sample text has {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b6be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test!', 'Or', 'is', 'this', 'not', 'a', 'test?', 'Test', 'it', 'to', 'be', 'sure.', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3420c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "  sep\n",
      "    The separator used to split the string.\n",
      "\n",
      "    When set to None (the default value), will split on any whitespace\n",
      "    character (including \\n \\r \\t \\f and spaces) and will discard\n",
      "    empty strings from the result.\n",
      "  maxsplit\n",
      "    Maximum number of splits.\n",
      "    -1 (the default value) means no limit.\n",
      "\n",
      "Splitting starts at the front of the string and works to the end.\n",
      "\n",
      "Note, str.split() is mainly useful for data that has been intentionally\n",
      "delimited.  With natural text that includes punctuation, consider using\n",
      "the regular expression module.\n",
      "\u001b[0;31mType:\u001b[0m      method_descriptor"
     ]
    }
   ],
   "source": [
    "str.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0ba08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefddcd1",
   "metadata": {},
   "source": [
    "Set takes the unique items from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "492c767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', '!', 'Or', 'is', 'this', 'not', 'a', 'test', '?', 'Test', 'it', 'to', 'be', 'sure', '.', ':', ')']\n",
      "['!', ')', '.', ':', '?', 'Or', 'Test', 'This', 'a', 'be', 'is', 'it', 'not', 'sure', 'test', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([.?!:()]|\\s)', text)\n",
    "tokens = [ items for items in tokens if items.split()]\n",
    "Stokens = sorted (list( set( tokens )) )\n",
    "print(tokens)\n",
    "print(Stokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b56ae35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('!', 0), (')', 1), ('.', 2), (':', 3), ('?', 4), ('Or', 5), ('Test', 6), ('This', 7), ('a', 8), ('be', 9), ('is', 10), ('it', 11), ('not', 12), ('sure', 13), ('test', 14), ('this', 15), ('to', 16)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = { token:index for index, token in enumerate( Stokens )}\n",
    "print( vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c3edff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"story.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "print(raw_text[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
