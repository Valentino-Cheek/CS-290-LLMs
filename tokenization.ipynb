{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f592fbb0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "_Tokenization_ is the process of converting a body of text into individual _tokens_, e.g., words and punctuation characters. This is the first step for most Natural Language Processing (NLP) tasks, including preparing data for training an LLM. Let's see how it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f17e49",
   "metadata": {},
   "source": [
    "## Some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc42f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test! Or is this not a test? Test it to be sure. :)\n",
      "This sample text has 61 characters.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test! Or is this not a test? Test it to be sure. :)\"\n",
    "print(text)\n",
    "print(f\"This sample text has {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b6be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test!', 'Or', 'is', 'this', 'not', 'a', 'test?', 'Test', 'it', 'to', 'be', 'sure.', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3420c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m str.split(self, /, sep=\u001b[38;5;28;01mNone\u001b[39;00m, maxsplit=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "  sep\n",
      "    The separator used to split the string.\n",
      "\n",
      "    When set to None (the default value), will split on any whitespace\n",
      "    character (including \\\\n \\\\r \\\\t \\\\f and spaces) and will discard\n",
      "    empty strings from the result.\n",
      "  maxsplit\n",
      "    Maximum number of splits (starting from the left).\n",
      "    -1 (the default value) means no limit.\n",
      "\n",
      "Note, str.split() is mainly useful for data that has been intentionally\n",
      "delimited.  With natural text that includes punctuation, consider using\n",
      "the regular expression module.\n",
      "\u001b[31mType:\u001b[39m      method_descriptor"
     ]
    }
   ],
   "source": [
    "str.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0ba08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefddcd1",
   "metadata": {},
   "source": [
    "Set takes the unique items from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492c767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', '!', 'Or', 'is', 'this', 'not', 'a', 'test', '?', 'Test', 'it', 'to', 'be', 'sure', '.', ':', ')']\n",
      "['!', ')', '.', ':', '?', 'Or', 'Test', 'This', 'a', 'be', 'is', 'it', 'not', 'sure', 'test', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([.?!:()]|\\s)', text)\n",
    "tokens = [ items for items in tokens if items.split()]\n",
    "Stokens = sorted (list( set( tokens )) )\n",
    "print(tokens)\n",
    "print(Stokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56ae35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('!', 0), (')', 1), ('.', 2), (':', 3), ('?', 4), ('Or', 5), ('Test', 6), ('This', 7), ('a', 8), ('be', 9), ('is', 10), ('it', 11), ('not', 12), ('sure', 13), ('test', 14), ('this', 15), ('to', 16)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = { token:index for index, token in enumerate( Stokens )}\n",
    "print( vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c3edff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b99049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PLUME of smoke drifted under the great glass dome of Slagborough Station as the Northern Express c\n"
     ]
    }
   ],
   "source": [
    "with open(\"one-foggy-night.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d22a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
