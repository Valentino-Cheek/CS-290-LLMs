{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUQFx9XSPhEOtoNcMLdTHO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_dR5-0pG_0CT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# needed to pull txt file from github\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"one-foggy-night.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/Valentino-Cheek/CS-290-LLMs/refs/heads/main/one-foggy-night.txt\")\n",
        "    file_path = \"one-foggy-night.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "TxTde7eyBO7C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read text file and print some text\n",
        "with open (\"one-foggy-night.txt\", \"r\" ) as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print (raw_text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQuVeLeXk0Hj",
        "outputId": "6bcf0a71-7c0b-4989-df4d-9c2e55f9b9bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A PLUME of smoke drifted under the great glass dom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initizating the tokenizer and encoding the text\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text = tokenizer.encode(raw_text)"
      ],
      "metadata": {
        "id": "ebysZGe3k4SZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "bC7uOhTqlIhd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "ZtW-ohbrmA0o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a dataloader with a max length of 4 and a stride of 1\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "second_batch = next(data_iter)\n",
        "\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgk4A-dlmDAW",
        "outputId": "4a73928a-b5bc-44ca-d7fc-8ad26882ae2b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   32,  9297, 38340,   286]]), tensor([[ 9297, 38340,   286,  7523]])]\n",
            "[tensor([[ 9297, 38340,   286,  7523]]), tensor([[38340,   286,  7523, 38648]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a data loader with a batch size of 2, max length of 6 and stride of 6\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=2, max_length=6, stride=6, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "# organize the data into inputs and targets\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hJ1mvtVmKeV",
        "outputId": "75082dc9-02c0-4410-d616-4c55245c1235"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   32,  9297, 38340,   286,  7523, 38648],\n",
            "        [  739,   262,  1049,  5405, 29500,   286]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 9297, 38340,   286,  7523, 38648,   739],\n",
            "        [  262,  1049,  5405, 29500,   286,  3454]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a data loader with a batch size, max length and stride of 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=4, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "# organize the data into inputs and targets\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTwLyfNnmcYq",
        "outputId": "5f9fb68b-b039-443e-9389-63ab59f58ee1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   32,  9297, 38340,   286],\n",
            "        [ 7523, 38648,   739,   262],\n",
            "        [ 1049,  5405, 29500,   286],\n",
            "        [ 3454,   363, 17913,  9327]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 9297, 38340,   286,  7523],\n",
            "        [38648,   739,   262,  1049],\n",
            "        [ 5405, 29500,   286,  3454],\n",
            "        [  363, 17913,  9327,   355]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterating through the tensors, looking at the words rather than the tokens\n",
        "for row in inputs:\n",
        "    print(tokenizer.decode ( row.tolist()))\n",
        "# iterating through the tensors, looking at the words rather than the tokens\n",
        "for row in targets:\n",
        "    print(tokenizer.decode ( row.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftscBqf0pamX",
        "outputId": "57dc65f2-50b3-4504-9787-dcf86b6d64eb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A PLUME of\n",
            " smoke drifted under the\n",
            " great glass dome of\n",
            " Slagborough Station\n",
            " PLUME of smoke\n",
            " drifted under the great\n",
            " glass dome of Sl\n",
            "agborough Station as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create an embedding layer with a vocab size of 8 and an output dimension of 4\n",
        "\n",
        "# Embedding is essential in the creation of LLMs because it allows the model\n",
        "# to have parameters that are trainable.\n",
        "\n",
        "#The embedding creates the ability for the model to interpret contextual\n",
        "#information through training.\n",
        "\n",
        "# Without it the model would not be able to interpret the english language,\n",
        "# embedding creates numbers that can be adjusted based on the raw words inputed.\n",
        "\n",
        "# This step allows (or is the foundation of ) the training and \"learning\" of a LLM\n",
        "vocab_size = 8\n",
        "output_dim = 4\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN-jrltoq8It",
        "outputId": "f038bc5c-272a-484e-8e37-4f796da4f366"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.6806, -0.4945, -1.7836, -1.4530],\n",
            "        [ 1.4918,  0.3406,  1.1078,  0.8309],\n",
            "        [ 0.2917, -1.3365, -0.4469,  0.2935],\n",
            "        [-1.1407,  0.4209, -0.0612, -0.6397],\n",
            "        [-0.9546,  0.2700, -0.3316,  0.0858],\n",
            "        [-0.2956, -0.0960, -0.7541,  0.7398],\n",
            "        [-1.8088, -1.9085,  0.7073,  0.1320],\n",
            "        [ 0.9494, -0.1660, -0.3525, -0.3045]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the requires_grad=True\n",
        "inputs = embedding_layer.weight.data\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUdgg64ftxli",
        "outputId": "37f779e5-e0b2-4dff-ab5a-fa3708c9f720"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6806, -0.4945, -1.7836, -1.4530],\n",
            "        [ 1.4918,  0.3406,  1.1078,  0.8309],\n",
            "        [ 0.2917, -1.3365, -0.4469,  0.2935],\n",
            "        [-1.1407,  0.4209, -0.0612, -0.6397],\n",
            "        [-0.9546,  0.2700, -0.3316,  0.0858],\n",
            "        [-0.2956, -0.0960, -0.7541,  0.7398],\n",
            "        [-1.8088, -1.9085,  0.7073,  0.1320],\n",
            "        [ 0.9494, -0.1660, -0.3525, -0.3045]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all of the attention scores by matrix multiplication\n",
        "# transpose in order to create a similarity matrix\n",
        "attention_scores = inputs @ inputs.T\n",
        "attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o89NJZxXylWw",
        "outputId": "94986137-79c9-4373-f227-269e6e013f77"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.9999, -2.3362,  1.2300,  0.0541, -0.3164,  0.1164, -1.7406,  1.7994],\n",
              "        [-2.3362,  4.2590, -0.2712, -2.1575, -1.6281, -0.6943, -2.4552,  0.7163],\n",
              "        [ 1.2300, -0.2712,  2.1572, -1.0557, -0.4660,  0.5962,  1.7457,  0.5669],\n",
              "        [ 0.0541, -2.1575, -1.0557,  1.8913,  1.1680, -0.1303,  1.1322, -0.9365],\n",
              "        [-0.3164, -1.6281, -0.4660,  1.1680,  1.1014,  0.5697,  0.9881, -0.8604],\n",
              "        [ 0.1164, -0.6943,  0.5962, -0.1303,  0.5697,  1.2125,  0.2820, -0.2241],\n",
              "        [-1.7406, -2.4552,  1.7457,  1.1322,  0.9881,  0.2820,  7.4318, -1.6901],\n",
              "        [ 1.7994,  0.7163,  0.5669, -0.9365, -0.8604, -0.2241, -1.6901,  1.1459]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the attention scores\n",
        "# soft max also highlights more important tokens, causes the model to \"focus\" on the most relevant tokens\n",
        "attention_weights = torch.softmax( attention_scores, dim = -1)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJeHCShuzoJI",
        "outputId": "1afdd77a-1250-4a27-9236-def80b9bd60f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.6960e-01, 2.3243e-04, 8.2234e-03, 2.5373e-03, 1.7517e-03, 2.7005e-03,\n",
              "         4.2165e-04, 1.4532e-02],\n",
              "        [1.2972e-03, 9.4898e-01, 1.0229e-02, 1.5509e-03, 2.6334e-03, 6.6998e-03,\n",
              "         1.1517e-03, 2.7460e-02],\n",
              "        [1.4801e-01, 3.2987e-02, 3.7410e-01, 1.5053e-02, 2.7149e-02, 7.8532e-02,\n",
              "         2.4790e-01, 7.6269e-02],\n",
              "        [6.7086e-02, 7.3474e-03, 2.2113e-02, 4.2122e-01, 2.0436e-01, 5.5790e-02,\n",
              "         1.9718e-01, 2.4913e-02],\n",
              "        [5.7594e-02, 1.5513e-02, 4.9592e-02, 2.5412e-01, 2.3776e-01, 1.3971e-01,\n",
              "         2.1228e-01, 3.3431e-02],\n",
              "        [9.7099e-02, 4.3163e-02, 1.5688e-01, 7.5869e-02, 1.5278e-01, 2.9055e-01,\n",
              "         1.1458e-01, 6.9074e-02],\n",
              "        [1.0306e-04, 5.0434e-05, 3.3662e-03, 1.8227e-03, 1.5781e-03, 7.7889e-04,\n",
              "         9.9219e-01, 1.0840e-04],\n",
              "        [4.0851e-01, 1.3831e-01, 1.1911e-01, 2.6487e-02, 2.8583e-02, 5.4002e-02,\n",
              "         1.2467e-02, 2.1253e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that each row of the normalized scores add up to 1, validating that softmax and the matrix multiplication went properly\n",
        "attention_weights.sum(axis = -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYRb1rg8z_mK",
        "outputId": "ef142fab-69af-4e9b-dfd9-730434a91065"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the context vectors based on the attention weights and inputs\n",
        "context_vectors = attention_weights @ inputs\n",
        "context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gipCkzQ0WCK",
        "outputId": "001b4545-2380-4165-d512-f3661e98f0b2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6703, -0.4923, -1.7404, -1.4100],\n",
              "        [ 1.4373,  0.3029,  1.0295,  0.7856],\n",
              "        [-0.1832, -1.0416, -0.3153, -0.0176],\n",
              "        [-0.9620, -0.2135, -0.1263, -0.2771],\n",
              "        [-0.8336, -0.3424, -0.1691, -0.0772],\n",
              "        [-0.2837, -0.4278, -0.4132,  0.1144],\n",
              "        [-1.7972, -1.8970,  0.6989,  0.1313],\n",
              "        [ 0.6249, -0.3595, -0.7465, -0.4813]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}