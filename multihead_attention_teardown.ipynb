{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUiZjleayubmhRSviykj+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valentino-Cheek/CS-290-LLMs/blob/main/multihead_attention_teardown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "BhzCF5OSi-RU"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "8CASKEfJilNA"
      },
      "outputs": [],
      "source": [
        "# # copited from LLMs-from-scratch/ch03/\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "#         super().__init__()\n",
        "#         assert (d_out % num_heads == 0), \\\n",
        "#             \"d_out must be divisible by num_heads\"\n",
        "\n",
        "#         self.d_out = d_out\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "#         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "#         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "#         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "#         self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.register_buffer(\n",
        "#             \"mask\",\n",
        "#             torch.triu(torch.ones(context_length, context_length),\n",
        "#                        diagonal=1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b, num_tokens, d_in = x.shape\n",
        "#         # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n",
        "#         # this will result in errors in the mask creation further below.\n",
        "#         # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "#         # do not exceed `context_length` before reaching this forward method.\n",
        "\n",
        "#         keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "#         queries = self.W_query(x)\n",
        "#         values = self.W_value(x)\n",
        "\n",
        "#         # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "#         # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "#         keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "#         values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "#         queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "#         # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "#         keys = keys.transpose(1, 2)\n",
        "#         queries = queries.transpose(1, 2)\n",
        "#         values = values.transpose(1, 2)\n",
        "\n",
        "#         # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "#         attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "#         # Original mask truncated to the number of tokens and converted to boolean\n",
        "#         mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "#         # Use the mask to fill attention scores\n",
        "#         attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "#         attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "#         attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "#         # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "#         context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "#         # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "#         context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "#         context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "#         return context_vec\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "\n",
        "# batch_size, context_length, d_in = batches.shape\n",
        "# d_out = 2\n",
        "# mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "# context_vecs = mha(batches)\n",
        "\n",
        "# print(context_vecs)\n",
        "# print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before your code review on Friday, be sure that your GitHub repo includes everything we've done in chapters 2 and 3. In particular,\n",
        "# your repo should include a notebook in which you implement multihead attention (as discussed in class and in the book) and do the following to explore how it works:\n",
        "\n",
        "#     Create a batch of inputs. This determines d_in and context_length.\n",
        "#     Decide on an output dimension, d_out.\n",
        "#     Pick a number of attention heads, num_heads, such that d_out is divisible by num_heads.\n",
        "#     In a sequence of cells, go step-by-step through the same calculation of queries and attention scores that the\n",
        "#     MultiHeadAttention class does. At each step, print the tensors and their shapes to see why reshaping is necessary.\n"
      ],
      "metadata": {
        "id": "RSgR_2GGipKh"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a random vector matrix with dimensions 6 x 10\n",
        "# i.e. four vectors of length 10\n",
        "d_in = 5\n",
        "context_length = 3\n",
        "\n",
        "inputs = torch.nn.Embedding( context_length, d_in )\n",
        "inputs = inputs.weight.data\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuiIYwhIphl4",
        "outputId": "785f75e1-f39b-4222-cd35-f07eb4019d51"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1025, -0.4555, -0.7091,  0.5402, -0.2880],\n",
              "        [ 3.3063,  1.0562, -0.5891, -0.5763,  0.0959],\n",
              "        [-1.0870,  0.2619,  0.4741, -1.2685,  1.2751]])"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create batches of this same input\n",
        "# results in 3 matrices which have 6 vectors of length 10 each\n",
        "batches = torch.stack((inputs, inputs, inputs), dim=0)\n",
        "print(batches)\n",
        "print(\"batches shape: \", batches.shape)\n",
        "# torch.stack?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch4cB9KWpmI9",
        "outputId": "e5e53058-ef7c-4369-8549-1c97420c655e"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.1025, -0.4555, -0.7091,  0.5402, -0.2880],\n",
            "         [ 3.3063,  1.0562, -0.5891, -0.5763,  0.0959],\n",
            "         [-1.0870,  0.2619,  0.4741, -1.2685,  1.2751]],\n",
            "\n",
            "        [[ 1.1025, -0.4555, -0.7091,  0.5402, -0.2880],\n",
            "         [ 3.3063,  1.0562, -0.5891, -0.5763,  0.0959],\n",
            "         [-1.0870,  0.2619,  0.4741, -1.2685,  1.2751]],\n",
            "\n",
            "        [[ 1.1025, -0.4555, -0.7091,  0.5402, -0.2880],\n",
            "         [ 3.3063,  1.0562, -0.5891, -0.5763,  0.0959],\n",
            "         [-1.0870,  0.2619,  0.4741, -1.2685,  1.2751]]])\n",
            "batches shape:  torch.Size([3, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define an output dimension and pick a number of attention heads\n",
        "# d_out msut be divisible by num_heads so that the later view call runs without error\n",
        "# if it was no the view call would fail because the new shape attempting to be created would have a different number of elements\n",
        "d_out = 8\n",
        "num_heads = 2\n",
        "# head dimensions = the length of each head once view is called\n",
        "head_dim = d_out // num_heads\n",
        "head_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwfb0b5bqMM0",
        "outputId": "63495669-af35-4317-e527-cfa7082c7e57"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the initial query and key weight matrices\n",
        "W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "W_keys = nn.Linear(d_in, d_out, bias=False)"
      ],
      "metadata": {
        "id": "CTGx0eY0slIf"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the queries and keys vectors\n",
        "keys = W_keys(batches)\n",
        "queries = W_query(batches)\n",
        "print(\"keys: \", keys, \"\\n keys shape: \",keys.shape)\n",
        "print(\"\\n queries :\", queries, \"\\n queries shape: \", queries.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1EXzLE4tKQJ",
        "outputId": "994c180a-35ce-4798-b156-9625c02f3fd8"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys:  tensor([[[ 0.4326, -0.5948, -0.3360, -0.1327, -0.9645,  0.5664,  0.2623,\n",
            "           0.7277],\n",
            "         [ 0.7467, -0.6135, -1.1176, -1.2574, -0.6431,  0.9303,  1.5005,\n",
            "           0.8179],\n",
            "         [-0.8376,  0.5426,  1.2401,  0.2090,  1.2661, -0.9023, -0.1516,\n",
            "          -1.1461]],\n",
            "\n",
            "        [[ 0.4326, -0.5948, -0.3360, -0.1327, -0.9645,  0.5664,  0.2623,\n",
            "           0.7277],\n",
            "         [ 0.7467, -0.6135, -1.1176, -1.2574, -0.6431,  0.9303,  1.5005,\n",
            "           0.8179],\n",
            "         [-0.8376,  0.5426,  1.2401,  0.2090,  1.2661, -0.9023, -0.1516,\n",
            "          -1.1461]],\n",
            "\n",
            "        [[ 0.4326, -0.5948, -0.3360, -0.1327, -0.9645,  0.5664,  0.2623,\n",
            "           0.7277],\n",
            "         [ 0.7467, -0.6135, -1.1176, -1.2574, -0.6431,  0.9303,  1.5005,\n",
            "           0.8179],\n",
            "         [-0.8376,  0.5426,  1.2401,  0.2090,  1.2661, -0.9023, -0.1516,\n",
            "          -1.1461]]], grad_fn=<UnsafeViewBackward0>) \n",
            " keys shape:  torch.Size([3, 3, 8])\n",
            "\n",
            " queries : tensor([[[-0.1259,  0.0497, -0.3309,  0.0205, -0.2086, -0.0671,  0.2306,\n",
            "           0.0199],\n",
            "         [-0.2754,  0.1565, -0.3743,  0.5242,  0.2479, -0.2026,  0.3662,\n",
            "           0.6087],\n",
            "         [ 0.6069, -0.3664,  0.0682,  0.1618,  0.0339,  0.6095, -0.4691,\n",
            "           0.3461]],\n",
            "\n",
            "        [[-0.1259,  0.0497, -0.3309,  0.0205, -0.2086, -0.0671,  0.2306,\n",
            "           0.0199],\n",
            "         [-0.2754,  0.1565, -0.3743,  0.5242,  0.2479, -0.2026,  0.3662,\n",
            "           0.6087],\n",
            "         [ 0.6069, -0.3664,  0.0682,  0.1618,  0.0339,  0.6095, -0.4691,\n",
            "           0.3461]],\n",
            "\n",
            "        [[-0.1259,  0.0497, -0.3309,  0.0205, -0.2086, -0.0671,  0.2306,\n",
            "           0.0199],\n",
            "         [-0.2754,  0.1565, -0.3743,  0.5242,  0.2479, -0.2026,  0.3662,\n",
            "           0.6087],\n",
            "         [ 0.6069, -0.3664,  0.0682,  0.1618,  0.0339,  0.6095, -0.4691,\n",
            "           0.3461]]], grad_fn=<UnsafeViewBackward0>) \n",
            " queries shape:  torch.Size([3, 3, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keys.view?"
      ],
      "metadata": {
        "id": "xXkKj0tbyqkq"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b = batch size / number of batches - 3\n",
        "# num tokens = number of vectors per batch - 3\n",
        "# d_in = the lenght of each vector / input dimensionality\n",
        "b, num_tokens, d_in = batches.shape\n",
        "print(b, num_tokens, d_in)\n",
        "\n",
        "# use view to tranform the keys and queries vectors\n",
        "# this takes the last dimension, d_out and splits it into two dimensions, depending on the num_heads wanted\n",
        "keys = keys.view(b, num_tokens, num_heads, head_dim)\n",
        "queries = queries.view(b, num_tokens, num_heads, head_dim)\n",
        "\n",
        "print(\"keys: \", keys, \"\\n keys shape: \",keys.shape)\n",
        "print(\"\\n queries: \", queries, \"\\n queries shape: \", queries.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN2aujtxt436",
        "outputId": "e72e8571-0130-4b13-cc3f-18c43ab411fe"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 3 5\n",
            "keys:  tensor([[[[ 0.4326, -0.5948, -0.3360, -0.1327],\n",
            "          [-0.9645,  0.5664,  0.2623,  0.7277]],\n",
            "\n",
            "         [[ 0.7467, -0.6135, -1.1176, -1.2574],\n",
            "          [-0.6431,  0.9303,  1.5005,  0.8179]],\n",
            "\n",
            "         [[-0.8376,  0.5426,  1.2401,  0.2090],\n",
            "          [ 1.2661, -0.9023, -0.1516, -1.1461]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4326, -0.5948, -0.3360, -0.1327],\n",
            "          [-0.9645,  0.5664,  0.2623,  0.7277]],\n",
            "\n",
            "         [[ 0.7467, -0.6135, -1.1176, -1.2574],\n",
            "          [-0.6431,  0.9303,  1.5005,  0.8179]],\n",
            "\n",
            "         [[-0.8376,  0.5426,  1.2401,  0.2090],\n",
            "          [ 1.2661, -0.9023, -0.1516, -1.1461]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4326, -0.5948, -0.3360, -0.1327],\n",
            "          [-0.9645,  0.5664,  0.2623,  0.7277]],\n",
            "\n",
            "         [[ 0.7467, -0.6135, -1.1176, -1.2574],\n",
            "          [-0.6431,  0.9303,  1.5005,  0.8179]],\n",
            "\n",
            "         [[-0.8376,  0.5426,  1.2401,  0.2090],\n",
            "          [ 1.2661, -0.9023, -0.1516, -1.1461]]]], grad_fn=<ViewBackward0>) \n",
            " keys shape:  torch.Size([3, 3, 2, 4])\n",
            "\n",
            " queries:  tensor([[[[-0.1259,  0.0497, -0.3309,  0.0205],\n",
            "          [-0.2086, -0.0671,  0.2306,  0.0199]],\n",
            "\n",
            "         [[-0.2754,  0.1565, -0.3743,  0.5242],\n",
            "          [ 0.2479, -0.2026,  0.3662,  0.6087]],\n",
            "\n",
            "         [[ 0.6069, -0.3664,  0.0682,  0.1618],\n",
            "          [ 0.0339,  0.6095, -0.4691,  0.3461]]],\n",
            "\n",
            "\n",
            "        [[[-0.1259,  0.0497, -0.3309,  0.0205],\n",
            "          [-0.2086, -0.0671,  0.2306,  0.0199]],\n",
            "\n",
            "         [[-0.2754,  0.1565, -0.3743,  0.5242],\n",
            "          [ 0.2479, -0.2026,  0.3662,  0.6087]],\n",
            "\n",
            "         [[ 0.6069, -0.3664,  0.0682,  0.1618],\n",
            "          [ 0.0339,  0.6095, -0.4691,  0.3461]]],\n",
            "\n",
            "\n",
            "        [[[-0.1259,  0.0497, -0.3309,  0.0205],\n",
            "          [-0.2086, -0.0671,  0.2306,  0.0199]],\n",
            "\n",
            "         [[-0.2754,  0.1565, -0.3743,  0.5242],\n",
            "          [ 0.2479, -0.2026,  0.3662,  0.6087]],\n",
            "\n",
            "         [[ 0.6069, -0.3664,  0.0682,  0.1618],\n",
            "          [ 0.0339,  0.6095, -0.4691,  0.3461]]]], grad_fn=<ViewBackward0>) \n",
            " queries shape:  torch.Size([3, 3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "# we do this so that when we do the dot product the number of heads is preserved while the number of tokens is in the dot product\n",
        "# keys and queries need to be multiplied along tokens per head\n",
        "# this is required because when you make the different heads by calling view num_heads is the 3rd dimension\n",
        "# you cannot form keys and queries correctly without view therefore this transpose is necessary\n",
        "# this enables the next step in which the dot product for head in each batch is calculated\n",
        "\n",
        "# we want each batch to have a number of heads that each have a number(context lengths/num_contexts) with a certain length(head_dim)\n",
        "# in the original shape each batch has a number of tokens that each have a head with a certain length\n",
        "keysT = keys.transpose(1, 2)\n",
        "queriesT = queries.transpose(1, 2)\n",
        "\n",
        "print(\"keys: \", keys, \"\\n keys shape: \",keys.shape)\n",
        "print(\"\\n queries: \", queries, \"\\n queries shape: \", queries.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPQr-vvWxRus",
        "outputId": "c9ec463e-da49-47e4-f796-87c1b0054485"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys:  tensor([[[[ 0.4326, -0.5948, -0.3360, -0.1327],\n",
            "          [-0.9645,  0.5664,  0.2623,  0.7277]],\n",
            "\n",
            "         [[ 0.7467, -0.6135, -1.1176, -1.2574],\n",
            "          [-0.6431,  0.9303,  1.5005,  0.8179]],\n",
            "\n",
            "         [[-0.8376,  0.5426,  1.2401,  0.2090],\n",
            "          [ 1.2661, -0.9023, -0.1516, -1.1461]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4326, -0.5948, -0.3360, -0.1327],\n",
            "          [-0.9645,  0.5664,  0.2623,  0.7277]],\n",
            "\n",
            "         [[ 0.7467, -0.6135, -1.1176, -1.2574],\n",
            "          [-0.6431,  0.9303,  1.5005,  0.8179]],\n",
            "\n",
            "         [[-0.8376,  0.5426,  1.2401,  0.2090],\n",
            "          [ 1.2661, -0.9023, -0.1516, -1.1461]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4326, -0.5948, -0.3360, -0.1327],\n",
            "          [-0.9645,  0.5664,  0.2623,  0.7277]],\n",
            "\n",
            "         [[ 0.7467, -0.6135, -1.1176, -1.2574],\n",
            "          [-0.6431,  0.9303,  1.5005,  0.8179]],\n",
            "\n",
            "         [[-0.8376,  0.5426,  1.2401,  0.2090],\n",
            "          [ 1.2661, -0.9023, -0.1516, -1.1461]]]], grad_fn=<ViewBackward0>) \n",
            " keys shape:  torch.Size([3, 3, 2, 4])\n",
            "\n",
            " queries:  tensor([[[[-0.1259,  0.0497, -0.3309,  0.0205],\n",
            "          [-0.2086, -0.0671,  0.2306,  0.0199]],\n",
            "\n",
            "         [[-0.2754,  0.1565, -0.3743,  0.5242],\n",
            "          [ 0.2479, -0.2026,  0.3662,  0.6087]],\n",
            "\n",
            "         [[ 0.6069, -0.3664,  0.0682,  0.1618],\n",
            "          [ 0.0339,  0.6095, -0.4691,  0.3461]]],\n",
            "\n",
            "\n",
            "        [[[-0.1259,  0.0497, -0.3309,  0.0205],\n",
            "          [-0.2086, -0.0671,  0.2306,  0.0199]],\n",
            "\n",
            "         [[-0.2754,  0.1565, -0.3743,  0.5242],\n",
            "          [ 0.2479, -0.2026,  0.3662,  0.6087]],\n",
            "\n",
            "         [[ 0.6069, -0.3664,  0.0682,  0.1618],\n",
            "          [ 0.0339,  0.6095, -0.4691,  0.3461]]],\n",
            "\n",
            "\n",
            "        [[[-0.1259,  0.0497, -0.3309,  0.0205],\n",
            "          [-0.2086, -0.0671,  0.2306,  0.0199]],\n",
            "\n",
            "         [[-0.2754,  0.1565, -0.3743,  0.5242],\n",
            "          [ 0.2479, -0.2026,  0.3662,  0.6087]],\n",
            "\n",
            "         [[ 0.6069, -0.3664,  0.0682,  0.1618],\n",
            "          [ 0.0339,  0.6095, -0.4691,  0.3461]]]], grad_fn=<ViewBackward0>) \n",
            " queries shape:  torch.Size([3, 3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "# keys. transpose 2,3 (b, num_heads, num_tokens, head_dim) -> (b, num_heads, head_dim, num_tokens)\n",
        "print(\"queries shape: \", queriesT.shape, \"\\nkeys transposed 2,3 shape: \",keysT.transpose(2, 3).shape)\n",
        "\n",
        "# Dot product for each head, specifically matrix multiplication is only 2D and @ defaults to the last 2 dimensions\n",
        "# in essence you're doing the dot product for each batches' heads\n",
        "attn_scores = queriesT @ keysT.transpose(2, 3)\n",
        "\n",
        "print(\"\\n\\n attn_scores: \", attn_scores, \"\\n\\n attn_scores shape: \",attn_scores.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J19jlVFO47bl",
        "outputId": "0b756f9e-480a-4b8a-bf47-23d5b19710ee"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries shape:  torch.Size([3, 2, 3, 4]) \n",
            "keys transposed 2,3 shape:  torch.Size([3, 2, 4, 3])\n",
            "\n",
            "\n",
            " attn_scores:  tensor([[[[ 0.0244,  0.2194, -0.2736],\n",
            "          [-0.1560, -0.5426, -0.0390],\n",
            "          [ 0.4361,  0.3983, -0.5888]],\n",
            "\n",
            "         [[ 0.2381,  0.4339, -0.2613],\n",
            "          [ 0.1851,  0.6994, -0.2565],\n",
            "          [ 0.4414,  0.1244, -0.8326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0244,  0.2194, -0.2736],\n",
            "          [-0.1560, -0.5426, -0.0390],\n",
            "          [ 0.4361,  0.3983, -0.5888]],\n",
            "\n",
            "         [[ 0.2381,  0.4339, -0.2613],\n",
            "          [ 0.1851,  0.6994, -0.2565],\n",
            "          [ 0.4414,  0.1244, -0.8326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0244,  0.2194, -0.2736],\n",
            "          [-0.1560, -0.5426, -0.0390],\n",
            "          [ 0.4361,  0.3983, -0.5888]],\n",
            "\n",
            "         [[ 0.2381,  0.4339, -0.2613],\n",
            "          [ 0.1851,  0.6994, -0.2565],\n",
            "          [ 0.4414,  0.1244, -0.8326]]]], grad_fn=<UnsafeViewBackward0>) \n",
            "\n",
            " attn_scores shape:  torch.Size([3, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mask, where every value north of the diagonal (not including the diagonal) is one\n",
        "# this stops the model from \"cheating\" i.e. looking ahead at the words that it shouldn't have access too\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "print(mask)\n",
        "\n",
        "\n",
        "# Original mask truncated to the number of tokens (ensures the dimensions match) and converted to boolean\n",
        "mask_bool = mask.bool()[:num_tokens, :num_tokens]\n",
        "print(mask_bool)\n",
        "\n",
        "# troubleshooting\n",
        "print(\"attn_scores shape:\", attn_scores.shape)\n",
        "print(\"mask_bool shape:\", mask_bool.shape)\n",
        "\n",
        "# Use the mask to fill attention scores\n",
        "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "print(attn_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89FHIDl52YC",
        "outputId": "c70a3e73-9c04-4294-e167-cbb0353c3502"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[False,  True,  True],\n",
            "        [False, False,  True],\n",
            "        [False, False, False]])\n",
            "attn_scores shape: torch.Size([3, 2, 3, 3])\n",
            "mask_bool shape: torch.Size([3, 3])\n",
            "tensor([[[[ 0.0244,    -inf,    -inf],\n",
            "          [-0.1560, -0.5426,    -inf],\n",
            "          [ 0.4361,  0.3983, -0.5888]],\n",
            "\n",
            "         [[ 0.2381,    -inf,    -inf],\n",
            "          [ 0.1851,  0.6994,    -inf],\n",
            "          [ 0.4414,  0.1244, -0.8326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0244,    -inf,    -inf],\n",
            "          [-0.1560, -0.5426,    -inf],\n",
            "          [ 0.4361,  0.3983, -0.5888]],\n",
            "\n",
            "         [[ 0.2381,    -inf,    -inf],\n",
            "          [ 0.1851,  0.6994,    -inf],\n",
            "          [ 0.4414,  0.1244, -0.8326]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0244,    -inf,    -inf],\n",
            "          [-0.1560, -0.5426,    -inf],\n",
            "          [ 0.4361,  0.3983, -0.5888]],\n",
            "\n",
            "         [[ 0.2381,    -inf,    -inf],\n",
            "          [ 0.1851,  0.6994,    -inf],\n",
            "          [ 0.4414,  0.1244, -0.8326]]]], grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHNjxUCDAqUS"
      },
      "execution_count": 204,
      "outputs": []
    }
  ]
}